# -*- coding: utf-8 -*-
"""JOB_UNIFICATION(PY_OPTIONALY).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-PbLlsP03UYLcQlSUwkQxOpOXMtf-7iQ
"""

import pandas as pd
import numpy as np
import pymorphy2
from transformers import BertTokenizer, BertModel
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import torch

def clean_text(text):
    if isinstance(text, str):
        # Удаление неразрывных пробелов \xa0
        text = text.replace('\xa0', ' ')

        # Удаление всех символов кроме букв и чисел
        text = re.sub(r'[^a-zA-Zа-яА-Я0-9]', ' ', text)

        # Удаление лишних пробелов
        text = re.sub(r'\s+', ' ', text).strip()

        text = text.lower()
        return text
    return text

df = pd.read_csv('path_to_your_file', encoding_errors= 'replace')

'''
+====+
Препроцессинг
+====+
'''

# удаление дубликатов
df = df.drop_duplicates()

df_next['job_title'] = df_next['job_title'].apply(lambda x: '' if isinstance(x, float) else x)

# очищение текста от разметок
df_next = df_next.apply(clean_text)

morph = pymorphy2.MorphAnalyzer()

'''
+=====+
Токенизация и эмбендинги
+=====+
'''
# Проверим, доступен ли GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Загрузка токенизатора и модели BERT
tokenizer = AutoTokenizer.from_pretrained("cointegrated/rubert-tiny2")
model = BertModel.from_pretrained("cointegrated/rubert-tiny2").to(device)

# Размер батча: можно уменьшить до 16, если память ограничена
batch_size = 16

# Путь для сохранения результатов
output_path = 'embeddings_output.csv'

# Открываем файл для записи результатов
with open(output_path, 'w') as f:
    f.write('job_title,embeddings\n')  # Заголовки для CSV

    # Цикл для батчевой обработки
    for i in range(0, len(df_next), batch_size):
        # Получаем батч данных
        batch_texts = df_next['job_title'].tolist()[i:i+batch_size]

        # Токенизация текстов
        tokens = tokenizer(batch_texts, padding=True, truncation=True, return_tensors="pt", max_length=128)

        # Перемещаем токены на устройство (GPU или CPU)
        tokens = {key: val.to(device) for key, val in tokens.items()}

        # Получение эмбеддингов с помощью модели BERT
        with torch.no_grad():
            outputs = model(**tokens)

        # Получение скрытых состояний
        hidden_states = outputs.last_hidden_state

        # Усреднение эмбеддингов токенов для каждого предложения
        sentence_embeddings = torch.mean(hidden_states, dim=1).cpu().numpy()

        # Для каждого предложения сохраняем результат
        for text, embedding in zip(batch_texts, sentence_embeddings):
            embedding_str = ' '.join(map(str, embedding))  # Преобразуем в строку
            f.write(f'"{text}","{embedding_str}"\n')

        # Печать прогресса
        if i % 10000 == 0:
            print(f"Обработано {i} записей из {len(df_next)}")

print(f"Результаты сохранены в {output_path}")

'''
+=====+
Кластеризация
+=====+
'''

batch_size = 10000  # Размер батча
num_clusters = 100  # Количество кластеров

kmeans = KMeans(n_clusters=num_clusters, random_state=42)

# Переменная для хранения результатов кластеров
cluster_results = []

processed_rows = 0

# Общий размер данных для контроля (можно вычислить заранее или получить из данных)
total_rows = sum(1 for row in open('embeddings_output.csv')) - 1  # Убираем заголовок

# Чтение батчами
for chunk in pd.read_csv('embeddings_output.csv', sep=',', chunksize=batch_size, on_bad_lines='skip'):
    # Выводим имена столбцов для диагностики
    print("Имена столбцов:", chunk.columns)

    # Проверяем, есть ли столбец 'embeddings' в данных
    if 'embeddings' not in chunk.columns:
        raise KeyError("Столбец 'embeddings' отсутствует в данных. Проверьте исходный файл.")

    # Преобразуем строковые эмбеддинги обратно в числовые массивы
    chunk['embeddings'] = chunk['embeddings'].apply(lambda x: np.fromstring(x, sep=' '))

    # Преобразуем эмбеддинги в массив numpy для кластеризации
    embeddings_matrix = np.vstack(chunk['embeddings'].values)

    # Применение K-Means для текущего батча
    chunk_clusters = kmeans.fit_predict(embeddings_matrix)

    # Добавляем метки кластеров к текущему батчу
    chunk['cluster'] = chunk_clusters

    # Сохраняем результаты кластеризации для данного батча
    cluster_results.append(chunk[['job_title', 'embeddings', 'cluster']])

    # Увеличиваем счетчик обработанных строк
    processed_rows += len(chunk)

    # Вывод прогресса
    print(f"Обработано {processed_rows} строк из {total_rows} ({(processed_rows / total_rows) * 100:.2f}%)")

# Объединяем все результаты кластеризации
df_results = pd.concat(cluster_results, ignore_index=True)

'''
+====+
Репрезентативные центроиды
+====+
'''

centroids = kmeans.cluster_centers_

# Найдем репрезентативные тексты для каждого кластера
representative_texts = []

for cluster_num in range(num_clusters):
    # Выбираем тексты, принадлежащие текущему кластеру
    cluster_texts = df_results[df_results['cluster'] == cluster_num]

    # Выбираем эмбеддинги этих текстов
    cluster_embeddings = np.vstack(cluster_texts['embeddings'].values)

    # Считаем расстояние от каждого эмбеддинга до центроида
    distances = np.linalg.norm(cluster_embeddings - centroids[cluster_num], axis=1)

    # Находим индекс самого близкого текста
    closest_idx = np.argmin(distances)
    # Добавляем репрезентативный текст для текущего кластера
    representative_texts.append(cluster_texts.iloc[closest_idx]['job_title'])
# Показательные значения для каждого кластера
for i, text in enumerate(representative_texts):
    print(f"Кластер {i}: {text}")

'''
+====+
Визуализация кластеров и центроидов
+====+
'''

pca = PCA(n_components=2)
all_embeddings = np.vstack(df_results['embeddings'].apply(np.array).values)
all_data_reduced = pca.fit_transform(all_embeddings)
centroids_reduced = pca.transform(centroids)

df_results[['pca_x', 'pca_y']] = all_data_reduced

plt.figure(figsize=(8, 6))
plt.scatter(df_results['pca_x'], df_results['pca_y'], c=df_results['cluster'], cmap='viridis', s=10, alpha=0.6, label='Профессии')
plt.scatter(centroids_reduced[:, 0], centroids_reduced[:, 1], c='red', s=200, marker='X', label='Центроиды')
plt.title('Кластеризация профессий с K-Means (PCA)')
plt.xlabel('Первая главная компонента')
plt.ylabel('Вторая главная компонента')
plt.legend()
plt.grid(True)
plt.show()